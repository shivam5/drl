<html>
 
 <head>
  <title>Deep Reinforcement Learning</title>
 </head>

 <body>
  
    <h1>Deep Reinforcement Learning</h1>
     


   <h3>Introduction</h3>

    <p>Reinforcement Learning is the branch of machine learning in which an agent learns by interacting with the environment.<br> 
    Perception, and representation of the environment is one of the key problems that must be solved before the agent can decide to select an optimal action to take. In reinforcement learning tasks, usually a human expert provides features of the environment based on his knowledge of the task. This causes the lack scalablity and is hence limited to fairly low-dimensional problems.<br>
    The powerful representation learning properties of deep neural networks has provided us with new tools to overcoming these problems, as they can overcome the curse of dimensionality, by automatically finding compact low-dimensional representations (features) of high-dimensional data (e.g., images, text and audio). 
    </p>
    <p>
    The first, major revolution in DRL, was the development of an algorithm that could learn to play seven Atari 2600 games from the Arcade Learning Environment, which represents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. <br>
    The second major success of DRL was the development of, AlphaGo, a DRL system that defeated a human world champion in Go.
    </p>
    
  <h3>Reinforcement Learning</h3>
    <p>
    Reinforcement Learning is mainly based on learning via interaction with the environment. At each step the agent interacts with the environment and learns the consequences of its actions via trial and error. The agent learns to alter its behaviour in response to the reward received due to its actions. <br>
    In RL, an autonomous agent is controlled by the machine learning algorithm and it observes a state s(t) at timestep t. It can then interact with the environment using an action a(t), reaching the state s(t+1) in the process. After reaching each new state, the agent receives a reward associated with that state. The aim of the agent is to find an optimal policy, which is a sequence of actions to reach the goal state and maximizing the rewards gained in the process. 
    However the problem faced is that the reward obtained at each state is unknown and the agent needs to learn the consequences of its actions by trial and error. Every interaction with the environment yields information, which the agent uses to update its knowledge.
    </p>
    A Reinforcement Learning problem can be represented using a Markov's Decision Process as follows:
    <ol>
    <li> A set of states S.</li>
    <li> Set of actions(per state) ùê¥.</li>
    <li> Transitions ùëá(ùë†, ùëé, ùë†‚Äô), mapping the state-action pair at time t to its resulting states.</li>
    <li> Reward function ùëÖ(ùë†, ùëé, ùë†‚Äô), representing the rewards for a particular transition.</li>
    <li> Discount Factor Œ≥ ‚àà [0,1], where lower values emphasize immediate rewards.</li>
     </ol>
    <br>
    If the MDP is episodic, i.e., the state is reset after each episode of length T, then the sequence of states, actions and rewards in an episode constitutes a trajectory of the policy. Every trajectory of a policy accumulates rewards from the environment, resulting in the return 
    R =PT ‚àí1t=0 Œ≥ t rt+1.

    The goal of RL is to find an optimal policy, œÄ‚àó , which achieves the maximum expected return from all states
    œÄ‚àó = argmaxœÄ E[R|œÄ].
    It is also possible to consider non-episodic MDPs, where T = ‚àû. In this situation, Œ≥ < 1 prevents an infinite sum of rewards from being accumulated.
    <p>
    A key concept underlying RL is the Markov property:
    <I>"Only the current state affects the next state, or in other words, the future is conditionally independent of the past given the present state."</I><br>
    This means that any decisions made at s(t) can be based solely on s(t‚àí1), rather than {s(0), s(1), . . . , s(t‚àí1)}.
    </p>
    <p>
    This assumption is somewhat unrealistic in different scenarios, as it requires the states to be fully observable.<br>
    A generalisation of MDPs are partially observable MDPs (POMDPs). A POMDP models an agent decision process in which the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.
    </p>
 
 <h3>Challenges in RL</h3>
    A summarization of the challenges faced by RL are as follows:
    <ul>
    <li>The optimal policy must be inferred by trial-and-error interaction with the environment as the rewards corresponding to transitions are unknown.
    <li>The observations of the agent depend on its actions and can contain strong temporal correlations.
    <li>Agents must deal with long-range time dependencies:<br>
    Often the consequences of an action only materialise after many transitions of the environment, but these cases are ignored due to the Markov Property.                                                                                       
    </ul>                                            
 
 <h2>Deep Reinforcement Learning techniques</h2>
 
 <h3>Asynchronous Methods for Deep Reinforcement Learning</h3>
 <p>
 Current Deep RL algorithms based on experience replay have shown a lot of success but they have some drawbacks such as higher memoury and computation requirements,
 and requirement of off-policy learning algorithms. This paper proposes an algorithm where we execute multiple agents asynchronously
 in parallel on multiple instances of the environment. Multiple agents can run different exploration policies on multiple threads.</p> 
 <b>Advantages of using Asynchronous Methods</b>
 <ol>
 <li>We do not use a replay memory and rely on parallel agents employing different exploration policies to perform the stabilizing role undertaken by experience replay in the
DQN training algorithm. Since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods.
 <li>A reduction in training time that is roughly linear in the number of parallel agents.
 <li>Previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs  
 , however this approach can run experiments on a single machine with a standard multi-core CPU.
</ol>
 
 When applied to a variety of Atari 2600 domains, on many games, asynchronous reinforcement learning achieves better results, in far less
time than previous GPU-based algorithms, using far less resources than massively distributed approaches.<br>
 The paper proposes asynchronous versions of 4 different reinforcement learning algorithms and shows that they
are able to train neural network controllers on a variety of domains in a stable manner.<br>
 The 4 asynchronous algorithms proposed are as follows:
 <ol>
  <li>Asynchronous one-step Q-learning
   <li>Asynchronous one-step Sarsa
    <li>Asynchronous n-step Q-learning
     <li>Asynchronous advantage actor-critic
  </ol>
 
 
 <h3>Deep reinforcement learning using Q-learning</h3>
 <h4>Introduction of Q-Learning</h4>
 <p>
 In this algorithm, the aim of the agent is to learn an optimal policy based on its interaction with the environment.
 The interaction can be specified with a history which is a sequence of state-action-rewards ‚ü®s0,a0,r1,s1,a1,r2,s2,a2,r3,s3,a3,r4,s4...‚ü©.
 which means that the agent was in state s0 and did action a0, which resulted in it receiving reward r1 and being in state s1, and so on.
 </p>
 <p>
  The aim of the agent is to learn a policy such that its value (usually the discounted reward) is maximized.<br>
  Q*(s,a), where a is an action and s is a state, is the expected value (cumulative discounted reward) of doing a in state s and then following the optimal policy. <br>
  Q-learning uses temporal differences to estimate the value of Q*(s,a). In Q-learning, the agent maintains a table of Q[S,A], where S is the set of states and A is the set of actions. Q[s,a] represents its current estimate of Q*(s,a). <br>
  An experience ‚ü®s,a,r,s'‚ü© provides one data point for the value of Q(s,a). The data point is that the agent received the future value of r+ Œ≥V(s'), where V(s') =maxa' Q(s',a'); this is the actual current reward plus the discounted estimated future value. This new data point is called a return. <br>
  The update for Q[s,a] is as follows:<br>
  Q[s,a] ‚Üê(1-Œ±) Q[s,a] + Œ±(r+ Œ≥maxa' Q[s',a']).
 </p>
 
 
 <h3>References</h3>
 <ol>
  <li>https://artint.info/html/ArtInt_265.html</li>
 </ol>
 
 </body>
</html>
