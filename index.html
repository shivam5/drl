<h1>Hello!</h1>
Deep Reinforcement Learning 


Introduction

<p>Reinforcement Learning is the branch of machine learning in which an agent learns by interacting with the environment.<br> 
Perception, and representation of the environment is one of the key problems that must be solved before the agent can decide to select an optimal action to take. In reinforcement learning tasks, usually a human expert provides features of the environment based on his knowledge of the task. This causes the lack scalablity and is hence limited to fairly low-dimensional problems.<br>
The powerful representation learning properties of deep neural networks has provided us with new tools to overcoming these problems, as they can overcome the curse of dimensionality, by automatically finding compact low-dimensional representations (features) of high-dimensional data (e.g., images, text and audio). 
</p>
<p>
The first, major revolution in DRL, was the development of an algorithm that could learn to play seven Atari 2600 games from the Arcade Learning Environment, which represents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. <br>
The second major success of DRL was the development of, AlphaGo, a DRL system that defeated a human world champion in Go.
</p>
Reinforcement Learning
<p>
Reinforcement Learning is mainly based on learning via interaction with the environment. At each step the agent interacts with the environment and learns the consequences of its actions via trial and error. The agent learns to alter its behaviour in response to the reward received due to its actions. <br>
In RL, an autonomous agent is controlled by the machine learning algorithm and it observes a state s(t) at timestep t. It can then interact with the environment using an action a(t), reaching the state s(t+1) in the process. After reaching each new state, the agent receives a reward associated with that state. The aim of the agent is to find an optimal policy, which is a sequence of actions to reach the goal state and maximizing the rewards gained in the process. 
However the problem faced is that the reward obtained at each state is unknown and the agent needs to learn the consequences of its actions by trial and error. Every interaction with the environment yields information, which the agent uses to update its knowledge.
</p>
A Reinforcement Learning problem can be represented using a Markov's Decision Process as follows:
<ol>
<li> A set of states S.</li>
<li> Set of actions(per state) ğ´.</li>
<li> Transitions ğ‘‡(ğ‘ , ğ‘, ğ‘ â€™), mapping the state-action pair at time t to its resulting states.</li>
<li> Reward function ğ‘…(ğ‘ , ğ‘, ğ‘ â€™), representing the rewards for a particular transition.</li>
<li> Discount Factor Î³ âˆˆ [0,1], where lower values emphasize immediate rewards.</li>
 </ol>
<br>
If the MDP is episodic, i.e., the state is reset after each episode of length T, then the sequence of states, actions and rewards in an episode constitutes a trajectory of the policy. Every trajectory of a policy accumulates rewards from the environment, resulting in the return 
R =PT âˆ’1t=0 Î³ t rt+1.

The goal of RL is to find an optimal policy, Ï€âˆ— , which achieves the maximum expected return from all states
Ï€âˆ— = argmaxÏ€ E[R|Ï€].
It is also possible to consider non-episodic MDPs, where T = âˆ. In this situation, Î³ < 1 prevents an infinite sum of rewards from being accumulated.
<p>
A key concept underlying RL is the Markov property:
<I>"Only the current state affects the next state, or in other words, the future is conditionally independent of the past given the present state."</I><br>
This means that any decisions made at s(t) can be based solely on s(tâˆ’1), rather than {s(0), s(1), . . . , s(tâˆ’1)}.
</p>
<p>
This assumption is somewhat unrealistic in different scenarios, as it requires the states to be fully observable.<br>
A generalisation of MDPs are partially observable MDPs (POMDPs). A POMDP models an agent decision process in which the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.
</p>
Challenges in RL
A summarization of the challenges faced by RL are as follows:
<ul>
<li>The optimal policy must be inferred by trial-and-error interaction with the environment as the rewards corresponding to transitions are unknown.
<li>The observations of the agent depend on its actions and can contain strong temporal correlations.
<li>Agents must deal with long-range time dependencies:<br>
Often the consequences of an action only materialise after many transitions of the environment, but these cases are ignored due to the Markov Property.                                                                                       
</ul>                                                                                        

