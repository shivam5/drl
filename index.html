<html>
 
 <head>
  <title>Deep Reinforcement Learning</title>
  <style>
   p#equation {
    height: 50px;
    line-height: 50px;
}

span {
    position: relative;
    font-size: 2.5em;
    display: inline-block;
    line-height: .7em;
    vertical-align: middle;
}

span:before {
    font-size: 12px;
    display: block;
    position absolute;
    left: 0;
    top: 0;
    content: "T-1";
    width: 22px;
    text-align: center;
}

span:after {
    font-size: 12px;
    display: block;
    position absolute;
    left: 0;
    bottom: 0;
    content: "t = 0";
    width: 27px;
    text-align: center;
}
  </style>
 </head>

 <body>
  
    <h1>Deep Reinforcement Learning</h1>
     


   <h3>Introduction</h3>
    <p>To give a very intuitive explanation of what reinforcement learning is. Imagine a baby that has come across a candle that is lit.
     So, out of curiosity the baby tries to touch the candle flame and will get hurt. But this experience will make him more cautious,
     and he will not repeat the action again. </p>
    <p>This is exactly how reinforcement learning works. An agent learns to do a particular job based on the previous experiences and outcome it receives.
     Like a child receives spanking and candies, the agent gets negative reward for wrong decisions and positive rewards for the right ones. This is basically reinforcement. </p>
     
    
  <h3>Reinforcement Learning (RL)</h3>
    <p>
    Reinforcement Learning (RL) is mainly based on learning via interaction with the environment. At each step the agent interacts with the environment and learns the consequences of its actions via trial and error. The agent learns to alter its behaviour in response to the reward received due to its actions. <br>
    In RL, an autonomous agent is controlled by the machine learning algorithm and it observes a state s(t) at timestep t. It can then interact with the environment using an action a(t), reaching the state s(t+1) in the process. After reaching each new state, the agent receives a reward associated with that state r(t+1). 
    The aim of the agent is to find an optimal policy. The policy is the strategy that the agent employs to determine the next action based on the current state. It maps states to actions, the actions that promise the highest reward.
 
     <img src="images/RL_schema.png" alt="Simple RL schema">
     <br>
    However the problem faced is that the reward obtained at each state is unknown and the agent needs to learn the consequences of its actions by trial and error. Every interaction with the environment yields information, which the agent uses to update its knowledge.
    </p>
    A Reinforcement Learning problem can be represented using a Markov's Decision Process as follows:
    <ol>
    <li> A set of states S. A state s is the situation in which the agent finds itself. For an agent who is learning to walk, the state would be position of its 2 legs. For an agent playing chess, the positions of all the pieces on the board would be the state.</li>
    <li> Set of actions(per state) ùê¥. An action is what an agent can do in each state. Given the agent who is learning to walk, the actions would include taking steps within a certain distance.</li>
    <li> Transitions ùëá(ùë†, ùëé, ùë†‚Äô), mapping the state-action pair at time t to its resulting states. It specifies the probablity that environment would transition to s', if the agent takes the action a when it is in state s.</li>
    <li> Reward function ùëÖ(ùë†, ùëé, ùë†‚Äô), representing the rewards for a particular transition. It is basically a feedback from the environment and is measures the success or failure of an agent's actions. For example, when mario touches some coins he wins positive reward.</li>
    <li> Discount Factor Œ≥ ‚àà [0,1], where lower values emphasize immediate rewards. It makes future rewards worth less than immediate rewards so that the agent does not delay necesarry actions.</li>
     </ol>
    <br>
    If the MDP is episodic, i.e., the state is reset after each episode of length T, then the sequence of states, actions and rewards in an episode constitute a trajectory of the policy. Every trajectory of a policy accumulates rewards from the environment, resulting in the return 
<p id = "equation">
   R= <span>&Sigma;</span>
 &gamma;<sup>t</sup>r<sub>t+1</sub>
</p>
<br>
  A policy is the strategy that the agent employs to determine the next action based on the current state. It maps a state to an action.
  <br>
  a = œÄ(s)
  <br>
  The goal of RL is to find an optimal policy, œÄ<sup>‚àó</sup> , which achieves the maximum expected return from all states
 <br> œÄ<sup>‚àó</sup> = argmax<sub>œÄ</sub> E[R|œÄ]<br>
    It is also possible to consider non-episodic MDPs, where T = ‚àû. In this situation, Œ≥ < 1 prevents an infinite sum of rewards from being accumulated.
    <p>
    A key concept underlying RL is the Markov property:
    <I>"Only the current state affects the next state, or in other words, the future is conditionally independent of the past given the present state."</I><br>
    This means that any decisions made at s<sub>t</sub> can be based solely on s<sub>t-1</sub>, rather than {s<sub>0</sub>, s<sub>1</sub>, . . . , s<sub>t-1</sub>}.
    </p>
    <p>
    This assumption is somewhat unrealistic in different scenarios, as it requires the states to be fully observable.<br>
    A generalisation of MDPs are partially observable MDPs (POMDPs). A POMDP models an agent decision process in which the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP.
    </p>
 

 <h3>Deep reinforcement learning (DRL)</h3>
     Perception, and representation of the environment is one of the key problems that must be solved before the agent can decide to select an optimal action to take. In reinforcement learning tasks, usually a human expert provides features of the environment based on his knowledge of the task. This causes the lack of scalablity and is hence limited to fairly low-dimensional problems.<br>
     <br>
 This is where the deep neural networks come in. The powerful representation learning properties of deep neural networks has provided us with new tools to overcoming these problems, as they can overcome the curse of dimensionality, by automatically finding compact low-dimensional representations (features) of high-dimensional data (e.g., images, text and audio). 
    </p>
    <p>
     DRL was first popularised by Gerry Tesauro at IBM in the early 1990s when he introduced the famous TD-Gammon program which combined neural networks with
     temporal-difference learning (explained later) to train an agent that would play world class backgammon.
     <br>
     The first, major revolution in DRL, was the development of an algorithm that could learn to play seven Atari 2600 games from the Arcade Learning Environment, which represents the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. <br>
    The second major success of DRL was the development of, AlphaGo, a DRL system that defeated a human world champion in Go.
    </p>
    <p>
    So, where do the neural networks fit in? In real life when we are the agents, we perceive the environment through our senses (eyes, nose, etc.), understand the state we are in, then we evaluate different actions and calculate the reward for each one of them.
     In DRL, neural networks are the agents that are responsible to map different sensory inputs to the output that would allow the optimal policy to be deduced. Neural networks use their coefficients to approximate the function realting the inputs to the outputs,
     it finds the right coefficients (or weights) by iteratively descending through the loss function gradients.
</p>
   <p>
    Deep neural network solves the problem of perception and representation of the environment, as convolutional networks are being used to recognize agent's state.
    So, the input would be just raw pixels to the convolutional network (for eg. the screen of a Mario game), the network would extract useful information from the image in order to calculate the reward or probability of the different actions possible. 
    Whether the agent directly outputs the probability of actions, or it ouputs the rewards for each state-action pair depends the algorithm we are using (policy gradient method or value iteration method). These methods would be discussed in detail in the later sections.
</p>
<img src="images/conv_agent.png" alt="DRL policy agent">
<p> The above image is an illustration of a policy agent. Given the input in raw pixels, it will output the probability of each action, i.e., it maps state to actions.</p>
Initially the weights of the network are initialized randomly. The learning takes place from the feedback of the environment, the neural network can use the difference between the ground truth reward (from the environment) and its expected reward (calculated by the network) as the loss function, 
and backpropagation through this loss function can be used to update the network weights to improve is interpretation.
</p>
<p> We would be discussing some of the papers of Deep Reinforcement Learning in detail in later sections to more deeply understand the methodology.</p>

<h4> Common terminologies </h4>
<p> We will explain some of the common terms that are used in the context of reinforcement learning that would be used in further discussions: </p>
<ul>

  <li><b>Value function (V) : </b>It is the expected long-term return (considering the discount factor). As opposed to the short-term reward given by reward function R. V<sub>œÄ(s)</sub> is the expected long-term return of the current state under policy œÄ. 
  
 <li><b>Q-value or action-value (Q): </b>It is similar to Value, except that it takes an extra parameter i.e., the current action a. Q<sub>œÄ(s, a)</sub> refers to the long-term return of the current state s, taking action a under policy œÄ. Q maps state-action pairs to rewards. 

 <li><b>Model based vs Model free reinforcement learning : </b>We represented RL problem as an MDP problem which can be characterised by (S,A,R,T). 
  So, if we know all the components of this MDP, we can compute the optimal policy without actual running in an environment. It would reduce to a planning problem, and there are classic planning algorithms for MDPs such as value iteration, policy iteration, etc.
  <br>
  But what makes the RL problem different from a planning problem is that the elements of the MDP are not usually known. More specifically the transition function T (how the environment would change in effect to an action), or the reward function R is not known a-priori. So, the agent has to perform action and observe the feedback from the environment.
  <br>
  So, if the agent doesn't know T and R, how will it find the optimal policy? There are 2 ways to do this:
  <ul>
   <li>Model based approaches : The agent can learn a model of the environment, i.e., it can learn T and R functions from its observations.
    That is, if the agent is currently in state s<sub>1</sub> and takes action a<sub>1</sub>, the environment transition to state s<sub>2</sub> given reward r<sub>2</sub>,
    this information will be used to improve the current estimate of T(s<sub>2</sub>|s<sub>1</sub>,a<sub>1</sub>) and R(s<sub>1</sub>,a<sub>1</sub>). Following this iterative approach, a model for the environment can be learned.
    After that, a planning algorithm can be used on the learned model to find a good policy. 
    <br>Such approaches which learn the model first are known as model-based RL algorithms.
    <li>Model free approaches : It turns out that it is not necessary to learn the model in order to learn a good policy. One of the most famous example of this approach would be that of
     Q-learning (explained in detail later) which directly estimates the Q-values i.e. the utility value of each action in each state through the experiences. A policy can then be derived by choosing the action with the highest Q-value in the current state.
     These approaches which learn a policy without actually learning the model are model-free approaches.
  </ul>
  
 <li><b>Off-policy vs on-policy learning : </b>There is a famous exploration vs exploitation dilemma in RL and an agent must strike
  a balance between the two phenomenon. Exploitation is to make the best decision given the current information, and exploration is gathering more information.
  <ul>
   <li>On-policy learning is analogous to explotation. In on-policy learning, the action which is sampled for learning (iterative updates) is the action from the current policy.
    <li>In off-policy learning, the action selected for the next iteration is not from the current policy. The policy used to generate behaviour, called the behaviour policy, may be unrelated to the policy that is evaluated and improved, called the estimation policy.
  </ul>
  
  
 <li><b>Temporal difference learning : </b>It is a central concept in reinforcement learning. Let us give an intuition for the concept.
  <br>
  Consider you want to make a model to predict the price of a particular stock. Finally, we want a prediction model which can predict what the price of the stock would be after an hour.
  The naive approach would be to make a prediction model for predicting the stock price after an hour, wait for an hour and update the model using the error.
  What temporal difference learning suggests is to not wait for an hour, but instead iteratively update the model by predicting the stock price every other minute.
  <br>
  We initialize the value function V at each state randomly. This is our initial estimate.
  After this we follow the policy œÄ, going from s<sub>t</sub> to s<sub>t+1</sub> and getting reward R<sub>t+1</sub>.
  We will update our value function V(s<sub>t</sub>) based on this one step reward (instead of waiting for termination and getting final reward), using simple update rule:
  <br>
  V(s<sub>t</sub>)‚ÜêV(s<sub>t</sub>)+Œ±(R<sub>t+1</sub>+Œ≥V(s<sub>t+1</sub>)‚àíV(s<sub>t</sub>))
  <br>
  The term (R<sub>t+1</sub>+Œ≥V(s<sub>t+1</sub>)‚àíV(s<sub>t</sub>)) is called the one-step temporal-difference error, 
  We keep updating our estimates of the value function till termination, and according to theory if we do this for a large number of time, we would converge to the optimal measure of the value function.
  
 <li><b>Policy gradient vs Q-learning : </b>
</ul>

 <h3>Challenges in RL</h3>
    A summarization of the challenges faced by RL are as follows:
    <ul>
    <li>The optimal policy must be inferred by trial-and-error interaction with the environment as the rewards corresponding to transitions are unknown.
    <li>The observations of the agent depend on its actions and can contain strong temporal correlations.
    <li>Agents must deal with long-range time dependencies:<br>
    Often the consequences of an action only materialise after many transitions of the environment, but these cases are ignored due to the Markov Property.                                                                                       
    </ul>                                            


 <h2>Deep Reinforcement Learning techniques</h2>
 
 <h3>Deep reinforcement learning using Q-learning</h3>
 <h4>Introduction of Q-Learning</h4>
 <p>
 In this algorithm, the aim of the agent is to learn an optimal policy based on its interaction with the environment.
 The interaction can be specified with a history which is a sequence of state-action-rewards ‚ü®s0,a0,r1,s1,a1,r2,s2,a2,r3,s3,a3,r4,s4...‚ü©.
 which means that the agent was in state s0 and did action a0, which resulted in it receiving reward r1 and being in state s1, and so on.
 </p>
 <p>
  The aim of the agent is to learn a policy such that its value (usually the discounted reward) is maximized.<br>
  Q<sup>*</sup>(s,a), where a is an action and s is a state, is the expected value (cumulative discounted reward) of doing a in state s and then following the optimal policy. <br>
  Q-learning uses temporal differences to estimate the value of Q*(s,a). In Q-learning, the agent maintains a table of Q[S,A], where S is the set of states and A is the set of actions. Q[s,a] represents its current estimate of Q*(s,a). <br>
  An experience ‚ü®s,a,r,s'‚ü© provides one data point for the value of Q(s,a). The data point is that the agent received the future value of r+ Œ≥V(s'), where V(s') =maxa' Q(s',a'); this is the actual current reward plus the discounted estimated future value. This new data point is called a return. <br>
  The update for Q[s,a] is as follows:<br>
  Q[s,a] ‚Üê(1-Œ±) Q[s,a] + Œ±(r+ Œ≥max<sub>a'</sub> Q[s',a']).
 </p>
 <h4>Playing Atari with Deep Reinforcement Learning</h4>
 <p>Mnih et al presented the first deep learning model (Playing Atari with Deep Reinforcement Learning) 
  leveraging reinforcement learning. The model
  directly learns optimal policy from high-dimensional sensory input (not any hand-crafted representation).
  They use a convolutional neural network which is trained using a variant of Q-learning. The input to their model
  is the raw pixels and output is the value function estimating future rewards.</p>
 <p>They applied their method to seven Atari 2600 games from the Arcade Learning Environment. This algorithm outperforms
  all previous approaches on six of the seven games, and outperforms humans in three of the games</p>

  <h3>Asynchronous Methods for Deep Reinforcement Learning</h3>
  <p>
  Current Deep RL algorithms based on experience replay have shown a lot of success but they have some drawbacks such as higher memoury and computation requirements,
  and requirement of off-policy learning algorithms. This paper proposes an algorithm where we execute multiple agents asynchronously
  in parallel on multiple instances of the environment. Multiple agents can run different exploration policies on multiple threads.</p> 
  <b>Advantages of using Asynchronous Methods</b>
  <ol>
  <li>We do not use a replay memory and rely on parallel agents employing different exploration policies to perform the stabilizing role undertaken by experience replay in the
 DQN training algorithm. Since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods.
  <li>A reduction in training time that is roughly linear in the number of parallel agents.
  <li>Previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs  
  , however this approach can run experiments on a single machine with a standard multi-core CPU.
 </ol>
  
  When applied to a variety of Atari 2600 domains, on many games, asynchronous reinforcement learning achieves better results, in far less
 time than previous GPU-based algorithms, using far less resources than massively distributed approaches.<br>
  The paper proposes asynchronous versions of 4 different reinforcement learning algorithms and shows that they
 are able to train neural network controllers on a variety of domains in a stable manner.<br>
  The 4 asynchronous algorithms proposed are as follows:
  <ol>
   <li>Asynchronous one-step Q-learning
    <li>Asynchronous one-step Sarsa
     <li>Asynchronous n-step Q-learning
      <li>Asynchronous advantage actor-critic
   </ol>
 
 <h3>References</h3>
 <ol>
  <li>https://artint.info/html/ArtInt_265.html</li>
  <li>Playing Atari with Deep Reinforcement Learning</li>
 </ol>
 
 </body>
</html>
